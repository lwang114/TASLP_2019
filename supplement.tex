\subsection{Acoustic feature extraction}
In the audio level, instead of phone sequence, we are given a sequence of acoustic feature vectors $\{\mathbf x_t\}_{t=1}^{T_x}$.  
Since features such as spectrogram or cochleagram tend to be high-dimensional, one essential step is to compress the audio features into lower dimensional space. Two main alternatives exist: one way is to employ knowledge in speech science to find a data-independent subspace, such as the mel-frequency subspace used in the MFCC ; another way is to find a data-dependent subspace using statistical models such as linear Gaussian models \cite{Roweis1997} or neural networks \cite{Fer2017}. The second approach is generally known as the \textit{bottleneck features}. The two approach can often be combined by, for example, extracting bottleneck features from MFCC.

The classical bottleneck feature can be viewed as the following linear Gaussian process:
\begin{align}\label{eq:bnf_gp}
    \mathbf z &\sim \mathcal N(0, \mathbf I)\\
    \mathbf w &\sim \mathcal N(0, \mathbf \epsilon \mathbf I)\\
    \mathbf  &= \mathbf C \mathbf z + \mathbf w,
\end{align}
where $\mathbf C \in \reals^{D \times B}$, and $B < D$ is the dimension of the bottleneck feature. The objective is to find $(\mathbf C, \mathbf\epsilon)$ such that $p(\mathbf x|\mathbf C, \epsilon)$ is optimized. Use the Baum-Welch auxiliary function:
\begin{align}\label{eq:lg_bnf_objective}
    &\max_{\bC, \epsilon} \mathbb E[\log p(\bx|\bz, \bC, \epsilon)|\bx, \bar\bC, \bar\epsilon]\\
    =&\max_{\bC, \Sigma}\mathbb E[-\frac{1}{2}\log (2\pi)^{B}|\Sigma| - \frac{1}{2}(\bx - \bC\bz) \Sigma^{-1}(\bx - \bC\bz)|\mathbf x, \bar\Theta].
\end{align}
As a result, the parameters of following iterative update:
\begin{align}
    \hat{\bC} &= x \mu_{\bz|\bx}^\top(\Sigma_{\bz|\bx} + \mu_{\bz|\bx}\mu_{\bz|\bx})^{-1}\\
    \hat{\Sigma} &= \mathbf E[(\bx - \bC \bz)(\bx - \bC \bz)^\top|\bx, \bar\Theta]\\
    &= \bx \bx^\top - \bC \mu_{z|x} \bx^\top,
\end{align}
where $\mu_{z|x} := \bC^\top(\bC \bC^\top + \Sigma)^{-1} \bx, \Sigma_{\bz|\bx} = \bI - \bC^\top(\bC \bC^\top + \Sigma)^{-1}\bC$. The bottleneck feature is then:
\begin{align}\label{eq:lg_bnf}
    f_{bnf}(x) = \mu_{z|x} = \bC^\top(\bC\bC^\top + \Sigma)^{-1} x.
\end{align}

\section{Multimodal word discovery with audio and image regions}
Theoretically, the multimodal framework can also be extended to deal directly with audio $x_1, \cdots, x_{T_x}$ and image regions containing single objects $y_1, \cdots, y_{T_y}$. In this case, again we assume there is one object for each image concept in each example but no longer know if two image regions from two examples correspond to the same object. There is also no knowledge about the total number of objects. The SMT model in this case combines the formalism of SMT Canonical Component Analysis (CCA) models.
\subsection{SMT model}
Analogous to the generative process for the discrete mixture model case, the audio-regional mixture model generates the image regions and audio as follows:
\begin{align}\label{eq:audio_region_gp}
    &a_t \sim \mathbf{Categorical}(\frac{1}{(1+T_y)}, \cdots, \frac{1}{(1+T_y)})\\
    &\pi \sim \mathbf{Dir}(\alpha)\\
    &c_i \sim \pi\\
    &\by_i \sim \mathcal N(\mu_{c_i}, \Sigma_{c_i}^\top), i=1,\cdots,T_y\\
    &n_i \sim \mathcal N(0, \Sigma_{n, c_i})\\
    &\bx_t = A \by_{a_t} + \bn, t=1,\cdots,T_x.
\end{align}
However, the EM-algorithm for this generative process will involve nonlinear coupling of the parameters for the audio and image regions. Instead, we employ an \textit{reparameterization} by introducing $\bz \sim \mathcal N(0, \bI)$ and $(C_x^i, C_y^i), i=1,\cdots,K$ such that $\bx_t := C_x^{c_{a_t}} \bz + \bw_i$ and $\by_i := C_y^{c_i}\bz + \bv_i$ for some normal noise variables with zero-mean and covariance $\Sigma_w, \Sigma_v$ respectively. This is equivalent to the original process by letting $A = C_x^{(c_{a_t})}C_y^{(c_{a_t})\top} (C_y^{c_{a_t}}C_y^{c_{a_t}}^\top+\Sigma_w)^{-1}, \Sigma_{n, c_i} = \bI - C_y^{(c_{a_t})\top} (C_y^{c_{a_t}}C_y^{c_{a_t}}^\top+\Sigma_w)^{-1}C_y + \Sigma_v$. 

% More about the equivalence
this serves to decouple the expression inside the expectation. The generative process now becomes:
\begin{align}\label{eq:audio_region_repara_gp}
    &a_t \sim \mathbf{Categorical}(\frac{1}{(1+T_y)}, \cdots, \frac{1}{(1+T_y)})\\
    &\pi \sim \mathbf{Dir}(\alpha)\\
    &c_i \sim \pi\\
    &\bz_i \sim \mathcal N(0, \bI)\\ 
    &\bw_i \sim \mathcal N(0, \Sigma_{w, c_i}), \bv_i \sim \mathcal N(0, \Sigma_{v, c_i})\\
    &\by_i \sim C_y^{c_i}\bz_i + \bw_i, i=1,\cdots,T_y\\
    &\bx_t = C_x^{c_i} \by_{a_t} + \bv_i, t=1,\cdots,T_x.
\end{align}

The posterior probability:
\begin{align*}
    p(\mathbf z|\mathbf x, \mathbf y) = \mathcal N(\mathbf z; \mu_{\mathbf z|\mathbf x, \mathbf y}, \Sigma_{\mathbf z|\mathbf x, \mathbf y}),
\end{align*}
where $\mu_{\mathbf z|\mathbf x, \mathbf y} := C_x^\top \Sigma_x^{-1}\mathbf x + C_y^\top \Sigma_y^{-1}(\mathbf y - C_y C_x^\top \Sigma_x^{-1}x), \Sigma_{\mathbf z|\mathbf x, \mathbf y} := I - C_x\top \Sigma_x^{-1} C_x $(continue).

\section{Prior works: unsupervised word discovery and speech-to-text tranlation}

\subsection{Unsupervised word discovery}
% Discussion of Lee & Glass's model
The first unsupervised word/subword discovery system was proposed by \cite{Lee2012}. Their model used the following generative process:
% TODO: two equations per line
\begin{align}\label{eq:generate_subword_hmm}
    \mathbf \pi &\sim \mathbf{Dir}(\gamma)\\
    \mathbf \theta_c &\sim \mathbf{Dir}(\theta_0), c \in \{1, \cdots, K\}\\
    \mathbf b_t &\sim \mathbf{Bernoulli}(\alpha_b), t \in \{1, \cdots, T\}\\
    d_i &= \left(\min_{t>t_{i-1}:b_t=1} t\right) - t_{i-1}\\
    c_i &\sim \mathbf \pi, i \in \{1, \cdots, L\}\\
    s_t &\sim \mathbf a^{c_i, s^{t-1}}\\
    \mathbf m_t &\sim \mathbf w^{c_i, s_t}\\
    \mathbf x_t &\sim \mathcal N(\mathbf \mu^{c_i, s_t}_{m_t}, (\mathbf \lambda^{c_i, s_t}_{m_t})^{-1}\mathbf I), \\
    t &\in \{t_{i-1}+1, \cdots, t_{i-1}+d_i\},
\end{align}
where $\mathbf \theta_c = [\mathbf a^{c_i}, \mathbf w^c, \mathbf \mu_t^{c}, \mathbf \lambda_i^c]$ and $t_{-1} = 0$, $\mathbf{Dir}(\theta)$ is the Dirichlet prior distribution with concentration parameter $\theta$ and $\mathbf{Bernoulli}(\alpha_b)$ is the Bernoulli distribution with probability $\alpha_b$ to be 1. Intuitively, the model first generates $K$ HMMs to model each subword cluster, and then for each utterance $\mathbf x$ it first generates a segmentation as specified by the duration vector $\mathbf d$ and model each segment of length $d_i$ with an HMM for cluster $c_i$. The model is trained using EM algorithm with Gibbs sampling. To infer the cluster id of each frame of the utterance, the model needs to find $\mathbf d, \mathbf c$ such that $p(\mathbf x, \mathbf d, \mathbf c)=p(\mathbf x, \mathbf d)p(\mathbf c|\mathbf x, \mathbf d)$  is maximized, which amounts to evaluate the posterior probability $p(\mathbf d_i, \mathbf x)$ and $p(\mathbf c_i|\mathbf x, \mathbf d)$. Due to prohibitive computational complexity to evaluate the expressions exactly, \cite{Lee2012} proposed to approximate the posterior with Gibbs sampling. The inference can be then essentially broken into two problems: first, finding an optimal segmentation of the utterance; second, finding the optimal cluster assignment given the segments of the utterance. The second problem is straightforward and amounts to finding the maximal posterior probabilities of the clusters; the first problem can be solved using a dynamic program with $\delta(t) := p(\mathbf y_{1:t}, z_{1:k(t)})$, where $k(t)$ is the cluster assigned to frame at time $t$:
\begin{align}\label{eq:subword_hmm_dp}
    \delta(t) &= \max_{\tau}\{\delta(t-\tau) (\sum_{c=1}^K\pi_c \sum_{i=1}^{N}\alpha^c_{\tau}(i))\} \\
    \phi(t) &= t - \arg \max_{\tau}\{\delta(t-\tau)\sum_{c=1}^K\pi_c \sum_{i=1}^{N}\alpha^c_{\tau}(i)\} \\
\end{align}
where $N$ is the number of state of the HMMs and $\alpha^c_(t)$ is the forward probability of HMM for cluster $c$. The end boundaries for the optimal segmentation is then $[1, \cdots, \phi(\phi(T)), \phi(T)]$.

% Discussion of Kamper 2016/2017 models
\cite{Kamper2017} has proposed a simplified Bayesian model that learns to jointly segment and cluster an audio waveform $\mathbf y = [y_1 \cdots y_T]$ into word-like unit $\mathbf z = z_1 \cdots z_K$. To that end, they employed an \textit{acoustic embedding} to map audio segments of variable duration into a fixed dimensional representation: $\mathbf x_i = f(\mathbf y_{t_1^i:t_2^i})$, where $i \in \{1, \cdots, M\}$ and $1 \leq t_1^i < t_2^i \leq T$. and $t_1^i \leq t_2^{i-1}, \forall i \in \{2, \cdots, M\}$. In the case where the entire audio consisting of spoken word, their system can be applied with full coverage and thus $t_1^i = t_2^{i-1}$. 

Their algorithm then iterates between finding a cluster assignment given segmentation and adjusting the segmentation given a cluster assignment. For a fixed segmentation, the acoustic embeddings are then modeled by the following GMM with Dirichlet prior for the cluster prior distributions and spherical normal for the cluster means: 
\begin{align}\label{eq:generate_bes_gmm}
    & \mathbf \pi \sim \mathbf{Dir}(a/K \mathbf 1)\\
    & \mu_c \sim \mathcal N(\mu_0, \sigma_0^2 \mathbf 1) \\
    & z_i \sim \mathbf \pi \\
    & \mathbf x_i \sim \mathcal N(\mu_{z_i}, \sigma^2 \mathbf I).
\end{align}
% More about the Gibbs sampling approach related to the model
The model can be trained using EM algorithm with Gibbs sampling and assign clusters by comparing the posterior probabilities $p(z_i=k|x_i)$. Given the clustering assignment, the model then updates the segmentation by performing the following dynamic program, analogous to \ref{eq:subword_hmm_dp}:
\begin{align}\label{eq:bes_gmm_dp}
    \delta(t) &= \max_{s}\{\delta(t-s) p(y_{t-s+1:t}|z_{k(t-s+1)})\} \\
    &= \max_s \{\alpha(t-s) p(f(y_{t-s+1:t}))^s\}\\
    \phi(t) &= t - \arg \max_{s}\{\delta(t-s) p(y_{t-s+1:t}|z_{k(t-s+1)})\} \\
    &= \max_s \{\delta(t-s) p(f(y_{t-s+1:t}))^s\}.
\end{align}

\section{Nonparametric Segmental Model}
While in the previous section, the alignment is assumed to be generated uniformly and exact inference with EM algorithm is used to estimate the GMM parameters. However, not only this is a local optimum, but also that the sparsity of the posterior probability may not be ensured. The uniform prior may be a reasonable assumption for phone alignments, but in the audio-level, since the feature vectors of the same concept can have much higher variability, uniform prior is more likely to create fragmented alignments. One may encourage sparsity of the alignment posterior probability $p(i(t)|\mathbf x, \mathbf y)$ with nonparametric Bayesian models. The idea is to add a weakly informative prior to the parameters of the model, $\theta$, so the posterior of the parameters is well-defined. Furthermore, priors like Dirichlet distribution on the alignment encourages sparsity of the probability by making the highly aligned concept even more likely.

In the case of audio-level word discovery, $\theta = (\pi, \mathbf c, \mathbf \mu)$, namely, the alignment prior $\mathbf \pi(T_y)$ and the mixture prior and means of the mixture model. For simplicity, the covariance of the mixture is assumed to be diagonal and fixed: $\Sigma(y) \equiv \sigma \mathbf I, y \in \mathcal Y$. A common choice is to use the conjugate priors of the data distribution:
\begin{align}\label{eq:parameter_prior}
   p(\pi(T_y)|\pi_0) &= \text{Dir}(\frac{\pi_0}{T_y+1})\\
   p(\mu|\mu_0) &= \mathcal N(\mathbf \mu; \mathbf \mu_0, \sigma_0)\\
\end{align}
where $\text{Dir}(\pi;\pi_0) := \frac{\Gamma(\pi_0 K)}{\prod_{k=1}^K\Gamma(\pi_0)}\prod_{k=1}^K\pi_k^{\pi_0-1}$. The parameter $\pi_0$ controls how much the prior depends on the initial uniform distribution, thus controlling the sparsity of the posterior. Let $\theta_0 = (\pi_0, \mu_0, \sigma_0)$, instead of maximizing Eq. (\ref{eq:audio_trans_prob}), the model tries to maximize:
\begin{align}
    p(\mathbf \theta|\mathbf x, \mathbf y, \theta_0) \propto p(\mathbf \theta|\theta_0) p(\mathbf x|\mathbf y, \mathbf \theta),
\end{align}
which in the Gibbs sampling framework, entails maximizing:
\begin{align}
    p(\theta|\mathbf x, \mathbf y, \mathbf s, \tilde{\mathbf A}, \theta_0),
\end{align}
where $\mathbf s \sim p(\mathbf s|\mathbf x, \mathbf y), \tilde{\mathbf A} \sim p(\cdot|\mathbf x, \mathbf y, \mathbf s)$. The alignment can be sampled as:
\begin{align}\label{eq:sample_alignment}
    p(i(s_j)|\tilde{\mathbf A}^-, \mathbf x, \mathbf y, \mathbf s, \theta_0) &\propto p(i(s_j), \mathbf x|\tilde{\mathbf A^-}, \mathbf y, \mathbf s, \theta_0) \\
    &= p(i(s_j)|\tilde{\mathbf A}^-, \mathbf y, \mathbf s, \theta_0) \\
    &p(\mathbf x_{s_j:s_{j+1}-1}|\mathbf x^-, \tilde{\mathbf A}, \theta_0), j = 1, \cdots, N.
\end{align}.
where $\tilde{\mathbf A}^-$ and $\mathbf x^-$ are the part of the assignment matrix and observation outside the segment $j$. Using the property of conjugate priors, the above and rest of the sampling equations are similar to those presented in \cite{Kamper2016, Kamper2017}.

% Discussion of Goddard et. al.'s system and prior works
\subsection{Speech-to-text translation}

 %\begin{align}\label{eq:segmm_sample_segment}
 %   l(t) &\sim p(l(t)|\mathbf{x}, \mathbf{y})\\
 %   &\propto p(\mathbf{x}, l(t)|\mathbf{y})\\
 %   &\propto p(\mathbf{x}_{1:t}, l(t)|\mathbf {y})\\
 %   &\propto \bar{\alpha}_t p(\mathbf{x}_{t-l(t)+1:t}|\mathbf y)\\
 %   &= \bar{\alpha}_t p(f_e(\mathbf{x}_{t-l(t)+1:t})|\mathbf y)^{l(t)},
%\end{align}
%where $\bar{\alpha}_t$ can be calculated recursively and $p(f_e(\mathbf{x}_{t-l(t)+1:t}|\mathbf y) = \sum_{i=1}^{T_y}\sum_{m=1}^{M}p(f_e(\mathbf{x}_{t-l(t)+1:t}, i, m|\mathbf y) \propto \sum_{i=1}^{T_y}\sum_{m=1}^{M}c_m(y_i)\mathcal N(f_e(\mathbf{x}_{t-l(t)+1:t})|\mu_m(y_i), \Sigma_m(y_i))$. 

 %Gibbs sampling  approximates Eq. (\ref{eq:baum-besgmm}):
%\begin{align}\label{eq:trans_prob_segmental_sampled}
%     p(i(t)|\mathbf y, \mathbf x) = \mathbb E_{\mathbf s \sim p(\mathbf s|\mathbf x, \mathbf y))}[p(i(t)|\mathbf x, \mathbf y, \mathbf s)]\\
%     \approx p(i(t)|\mathbf x, \mathbf y, \bar{\mathbf s}).
%\end{align} 


%One approach proposed by \cite{Kamper2017} is to assume that the segmentation to be equally likely. Define $\mathbf b \in \{0, 1\}^{T_x}$ to be the \textit{boundary vector} such that $b_{s_j}=1, j=1,\ldots,N$ and 0 otherwise. Then then in their model, $p(b_t|\mathbf y) = \frac{1}{2}$ and $p(\mathbf s|\mathbf y)=p(\mathbf b|\mathbf y) = \frac{1}{2^{T_x}}$. \cite{Lee2012} makes a more general assumption that $p(b_t=1|\mathbf y) = \alpha_b$ and $p(\mathbf s|\mathbf y)=p(\mathbf b|\mathbf y) = \frac{1}{\prod_{t=1}^{T_x}\alpha_b^{b_t}(1-\alpha_b)^{1-b_t}}$. As a result, the number of segments per sentence will obey a binomial distribution with $(T_x, \alpha_b)$. 

%With the additional information from the image concepts, the above expression for number of segments can be generalized by having $\alpha_b$ depending on the number of concepts. Intuitively, when there are more concepts, there should be more segments and $\alpha_b$ should be larger. For example, we can let $\alpha_b(T_y, T_x) = \frac{\alpha T_y}{T_x}$ for some constant $\alpha$ such that $\alpha T_y \leq T_x$ and
%\begin{align}\label{eq:concept_dependent_n_segment}
%    p(N|\mathbf y) = \text{Binomial}(N; T_x, \frac{\alpha T_y}{T_x}),
%\end{align}
%where $\text{Binomial}(m;n, p) := \left(\begin{array}{c}
%     n  \\
%     m 
%\end{array}\right)p^m(1-p)^{n-m}$. It is also possible to make $\alpha_b$ concept dependent, similar to the fertility approach used in \cite{Brown93}, but it will requires more parameters. Besides, the support of the prior can be further shrunk by restricting the length of a single segment with prior knowledge about the range of word length, though this will make the approach language-dependent. 

%Plug Eq. (\ref{eq:concept_dependent_n_segment}), the conditional independence assumption into Eq. (\ref{eq:audio_trans_prob}) yields:
%\begin{align}\label{eq:audio_trans_prob_simplified}
%    &\sum_{N=0}^{2\alpha \frac{T_y}{T_x}}\sum_{\mathbf s \in \mathcal S_N}\alpha(T_y, T_x)^N (1-\alpha(T_y, T_x))^{T_x-N}\nonumber\\
%    &\frac{\epsilon}{(T_y+1)^{T_x}}\prod_{j=1}^{N}\sum_{i(s_j)=1}^{T_y}p(\mathbf x_{s_j:s_{j+1}-1}|y_{i(s_j)}).
%\end{align}

%The modeling problem then boils down to model the segment probabilities $p(\mathbf x_{s_j:s_{j+1}-1}|y_{i(s_j)})$. while a single phone occupies only one frame in the phone-level discovery, audio segment for the same phone/word may occupy variable frames. Besides modeling each segment using HMM \cite{Lee2012}, one existing approach \cite{Kamper2017} is to embed each segment into a fixed-dimensional space, so problem reduces to the phone-level discovery. Let the \textit{embedding vector} of the $j$-th segment be $v_j = f_e(x_{s_{j-1}:s_j}), s_0:=1$.

%Suppose the segmentation is known, Eq. (\ref{eq:smt_trans_prob}) becomes:
%\begin{align}\label{eq:segmental_smt_trans_prob}
%    p(\mathbf{x}, \mathbf{s}|\mathbf{y}, \tilde{\mathbf{A}})
%    &= \sum_{\tilde{\mathbf A}} p(\mathbf{v}|\mathbf {y}, \tilde{\mathbf A})\\
%    &=\prod_{j=1}^N p(v_j|y_{i(j)}),
%\end{align}
%where $\tilde A \in \{0, 1\}^{N \times T_y}$ is simply the alignment matrix $A$ with rows that corresponds to the same segment collapse into a single row. which essentially reduces to the discrete case.

%However, in order to optimize Eq. (\ref{eq:trans_prob_segmental}) or even the Baum auxiliary function, marginalizing over all possible segmentations is required when calculating the posterior of the alignment given the features and the image concepts, $p(i(t)|\mathbf x, \mathbf y), t=1,\cdots, T_x$. One way to get around is to use Gibbs sampling by constructing an unbiased estimator of Eq. (\ref{eq:audio_trans_prob}):
%\begin{align}\label{eq:audio_trans_prob_sampled}
%     p(i(t)|\mathbf y, \mathbf x) = \mathbb E_{\mathbf s \sim p(\mathbf s|\mathbf x, \mathbf y))}[p(i(t)|\mathbf x, \mathbf y, \mathbf s)]\\
%     \approx p(i(t)|\mathbf x, \mathbf y, \bar{\mathbf s}),
%\end{align}
%where $\bar{\mathbf{s}} \sim p(\bar{\mathbf{s}}=\cdot |\mathbf x, \mathbf y))$.

%Following \cite{Kamper2017}, we randomly sample a segmentation according to the distribution $p(\mathbf s|\mathbf x, \mathbf y)$ using a collapsed Gibbs sampler: Let $\bar{\alpha}_t := \sum_{i=1}^{T_y}\alpha_t(i)$, where $\alpha_t(i)$ is the forward variable defined similarly to the phone-level case as $p(\mathbf{x}_{1:t}, i(t)=i|\mathbf{y})$, and let $l(t)$ denotes the length of the candidate segment that ends at time t, then the segmentation can be segmented backward from $t=T_x$ as:
%Let $\gamma_t(i, m|\mathbf s) = p(i(t)|\mathbf x, \mathbf y, \mathbf s)$, the parameters of the GMM as:
%\begin{align}\label{eq:segmm_em_update}
%    \hat c_m(i) &= \frac{\sum_{t=1}^{T_x}\gamma_t(i, m|\mathbf s)}{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m|\mathbf s)}\\
%    \hat \mu_m(y_i) &= \frac{\sum_{t=1}^{T_x}\gamma_t(i, m|\mathbf s)f_e(x_{s_{i}:s_{i+1}-1})}{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m|\mathbf s)}\\
%    \hat \Sigma_m(y_i) &= 
%    \frac{\sum_{t=1}^{T_x}\gamma_t(i, m|\mathbf s)}{\sum_{t=1}^{T_x}\sum_{m=1}^{M}\gamma_t(i, m|\mathbf s)}\\
%    &(f_e(\mathbf x_{s_{i-1}+1:s_{i}}) - \mathbf \mu_m(y_i))(f_e(\mathbf x_{s_i:s_{i+1}-1}) - \mathbf \mu_m(y_i))^\top.
%\end{align}

% Discuss the use of Dirichlet prior for image concepts

%Existing works on acoustic word discovery has modeled variable-length context by explicitly modeling the segmentation in the generative process. However, since the word segmentation is generally assumed to be not directly observable in the acoustic feature, certain prior assumption about the word length distribution is often used. \cite{Lee2012} models the length probability as:
%\begin{align}\label{eq:len_prob_seg_hmm}
%    p(l_i=l|\mathbf z_i, l_1^{i-1}) = &\frac{(1 - \alpha_{z_i})^l \alpha_i}{\sum_{l'=1}^{T_x - \sum_{i'=1}^{i-1}l_{i'}}(1 - \alpha_i)^l \alpha_i},\\
%    1\leq l \leq T_x - \sum_{i'=1}^{i-1}l_{i'},
%\end{align}
%which is simply the length distribution of $i-th$ segment starting with $1$ and continuing with $0$ of a binary string generated by $i.i.d$ Bernoulli random variables with parameter $\alpha_{z_i}$. 
